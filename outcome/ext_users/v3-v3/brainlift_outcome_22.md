# Prompt Engineering

**Owner:** Huzefa  
**Assessment Date:** 2025-08-29

---

## Purpose Statement

### Issue 1: Insufficient Business Context and Problem Definition

#### Feedback

The purpose statement "To contain information on designing effective prompts for LLMs. This is so we can eliminate guess work in generating prompts and get precise, accurate outputs" lacks critical elements. It doesn't explain why prompt engineering expertise matters now, what business variables will be maximized by solving this problem, or provide measurable outcomes.

#### Coaching

Rewrite your purpose to answer: What specific business problem does poor prompting cost you? How will you measure success - reduced iteration time, increased output quality scores, cost savings? Add context about why this expertise is urgent in your current role or industry.

### Issue 2: Missing Scope Definition

#### Feedback

Your purpose statement doesn't define what's included or excluded from this BrainLift's domain. This creates confusion about whether you're covering all LLM prompting or focusing on specific use cases.

#### Coaching

Define boundaries clearly: "In scope: [specific prompting techniques, use cases, or domains]. Out of scope: [what you won't cover]." This prevents scope creep and helps you build focused expertise.

### Issue 3: No Measurable Outcomes

#### Feedback

The statement mentions "precise, accurate outputs" but provides no specific metrics or success criteria. This makes it impossible to evaluate whether your BrainLift is achieving its stated goal.

#### Coaching

Replace vague terms with specific metrics: "Reduce prompt iteration cycles by 50%" or "Achieve 90% first-attempt success rate for [specific task type]." Include who benefits and how they'll measure improvement.

## Experts

### AI Rabbit

### Issue 1: Missing Strategic Perspective Analysis

#### Feedback

AI Rabbit is listed only with basic information - Medium link and "Practical advice on LLM tools." You haven't analyzed their main views, SPOVs, or explained why following them specifically helps solve your prompting problem.

#### Coaching

Add AI Rabbit's key positions on prompt engineering: Do they favor structured prompting vs. conversational? What techniques do they advocate? Include what you agree/disagree with and how their perspective differs from other experts.

### Huzefa Tahir

### Issue 1: Self-Reference Without External Validation

#### Feedback

Listing yourself as an expert without external validation or clear positioning relative to other domain experts weakens credibility. The description "trying to work with an LLM first approach" suggests learning rather than expertise.

#### Coaching

Either remove yourself or reframe your inclusion: What unique experience or results do you bring? If you're the practitioner implementing others' insights, position yourself as the "application expert" while citing the theoretical experts you follow.

### Issue 2: Insufficient Expert Diversity

#### Feedback

Two experts aren't sufficient for a domain as broad as prompt engineering. You're missing academic researchers, enterprise practitioners, and different schools of thought, creating an echo chamber risk.

#### Coaching

Add 3-5 more experts representing different perspectives: academic researchers (like those from OpenAI, Anthropic), enterprise practitioners, and critics of current approaches. Include experts you disagree with to avoid confirmation bias.

## DOK1 Facts

### LLMs have limited context lengths

### Issue 1: Lack of Specificity and Source Attribution

#### Feedback

This fact is too generic and lacks supporting details. Different LLMs have different context lengths, and this limitation affects prompting strategies differently. No source attribution makes it unverifiable.

#### Coaching

Specify exact context lengths for models you use: "GPT-4 has 128K tokens, Claude has 200K tokens [source link]." Explain how these limits impact your specific prompting strategies rather than stating obvious limitations.

### LLM's recall depends on where in the context window the information was.

### Issue 1: Missing Critical Details

#### Feedback

This fact touches on an important phenomenon but lacks the specificity needed for practical application. The "lost in the middle" research has specific findings about attention patterns that would make this fact actionable.

#### Coaching

Expand with research details: "Information in middle sections of long contexts shows 20% lower recall than information at start/end [source: Lost in the Middle paper]." Include practical implications for prompt structure.

### LLM's take the easy way out.

### Issue 1: Vague and Unsupported Claim

#### Feedback

This statement is too vague to be useful. "Easy way out" could mean many things - following common patterns, avoiding complex reasoning, or defaulting to training biases. Without specifics, it doesn't inform prompting decisions.

#### Coaching

Replace with specific, sourced behaviors: "LLMs default to common response patterns when instructions are ambiguous, reducing output uniqueness by 40% [source needed]." Focus on documented behavioral patterns that impact prompt design.

## DOK2 Summaries

### You can tell it to not make any changes but after 5 messages it will go in and make a change.

### Issue 1: Lacks Logical Foundation and Context

#### Feedback

This summary appears disconnected from any underlying facts or source analysis. It doesn't explain WHY this happens or HOW it relates to prompt engineering principles. The observation lacks context about which models, what types of changes, or underlying mechanisms.

#### Coaching

Connect this observation to underlying facts: What causes this behavior - context drift, instruction following degradation, or training patterns? Link to specific sources and explain the mechanism so you can design prompts that account for this limitation.

### Issue 2: Insufficient Summary Coverage

#### Feedback

One summary is inadequate for a BrainLift on prompt engineering. You need summaries that synthesize multiple facts from sources to explain patterns in LLM behavior, effective prompting techniques, and their applications.

#### Coaching

Create 5-7 summaries covering key areas: structured prompting techniques, few-shot learning patterns, instruction hierarchy effectiveness, and output consistency methods. Each summary should synthesize multiple facts from your sources.

## DOK3 Insight

### Issue 1: Missing Strategic Analysis

#### Feedback

Your DOK3 Insights section is completely empty. This means you haven't developed any practical rules of thumb from analyzing patterns across your sources. Without insights, you can't form actionable SPOVs or demonstrate expert-level thinking.

#### Coaching

Develop 3-4 insights by analyzing patterns across your facts and summaries. Example: "Prompt specificity beats prompt length - detailed constraints in 50 words outperform verbose explanations in 200 words." Ground each insight in multiple sources and make them actionable.

## DOK4 SPOVs

### Issue 1: Complete Absence of Decision Framework

#### Feedback

Your SPOVs section is empty, meaning you have no actionable decision rules or controversial stances. Without SPOVs, this BrainLift won't guide your decisions or differentiate your approach from standard prompting advice available everywhere.

#### Coaching

Create 2-3 controversial, actionable SPOVs based on your insights. Example: "Always optimize prompts for worst-case scenarios, not average performance - design for the 10% edge cases that break most prompts." Make them specific enough that others could disagree and implementable in your daily work.

---

**Critical Assessment**: This BrainLift is essentially a skeleton structure with minimal content across all DOK levels. The fundamental question is whether developing prompt engineering expertise justifies the effort when abundant resources already exist. Consider whether your time would be better spent applying existing prompt engineering resources rather than building custom expertise from scratch.
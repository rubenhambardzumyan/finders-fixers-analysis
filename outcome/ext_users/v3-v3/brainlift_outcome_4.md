# AI-Powered Codebase Onboarding for Maximum Productivity

**Owner:** Bach Ly  
**Assessment Date:** 2025-08-28

---

## Purpose Statement

### Issue 1: Vague Problem Definition

#### Feedback

The purpose mentions "cutting 50% of onboarding toil" but doesn't define what constitutes "toil" or establish baseline measurements. Without knowing current onboarding times or specific pain points, the 50% target is meaningless.

#### Coaching

Define specific metrics: "Reduce time from repository clone to first productive commit from current average of 2 weeks to 1 week." Identify exactly what activities consume time - documentation reading, architecture exploration, or domain knowledge acquisition. Measure current state before building expertise.

### Issue 2: Unclear Business Context

#### Feedback

Missing context about why Trilogy needs this expertise now versus using existing AI tools directly. The problem doesn't explain organizational constraints or what makes this different from standard developer tool adoption.

#### Coaching

Explain Trilogy's specific constraints: legacy codebases, proprietary architecture patterns, or compliance requirements that make standard AI tools insufficient. If no unique constraints exist, consider whether this BrainLift is necessary or if tool pilots would be more effective.

### Issue 3: Undefined Success Criteria

#### Feedback

No measurable business impact beyond the 50% time reduction. Missing context about affected teams, cost savings, or productivity gains that would justify developing custom expertise versus hiring consultants or using existing solutions.

#### Coaching

Quantify business impact: "Reduce onboarding costs by $X per engineer" or "Enable 30% faster team scaling." Define who benefits - new hires, managers, or technical leads. Without clear ROI, this effort may not be worth the investment.

## Experts

### McKinsey

#### Issue 1: Generic Corporate Source

#### Feedback

McKinsey is cited as a company source but their main views on AI-driven onboarding aren't specified. Corporate consulting firms provide general insights, not domain-specific expertise for developer onboarding challenges.

#### Coaching

Either find specific McKinsey researchers who focus on developer productivity (not general AI adoption) or replace with practitioners who actually implement onboarding solutions. You need experts who understand code architecture discovery, not just AI productivity metrics.

### Sourcegraph

#### Issue 1: Missing Specific Viewpoints

#### Feedback

Described as publishing "deep dives on embedding-based code search" but no specific controversial positions or methodologies are identified. This reads like marketing copy rather than expert analysis.

#### Coaching

Research Sourcegraph's specific positions on semantic search versus keyword search, their views on AI code analysis accuracy, or their stance on centralized versus distributed code intelligence. What do they disagree with other vendors about?

### Hamel Husain

#### Issue 1: Duplicate Description

#### Feedback

The description for Hamel Husain is identical to Sourcegraph's description, indicating copy-paste errors and lack of individual expert research.

#### Coaching

Research Hamel's actual expertise - his work on ML infrastructure, his positions on AI evaluation methodologies, or his controversial takes on AI tool effectiveness. Don't duplicate descriptions between experts.

### Shawn Wang

#### Issue 1: Superficial Understanding

#### Feedback

Described as framing "AI Engineer" as a new craft but no specific insights about his onboarding methodologies or controversial positions on AI tool selection are provided.

#### Coaching

Identify Shawn's specific views on prompt engineering for code exploration, his positions on AI agent orchestration in development workflows, or his disagreements with traditional onboarding approaches. What makes his "AI Engineer" concept actionable for onboarding?

## DOK1 Facts

### McKinsey AI Productivity Facts

#### Issue 1: Missing Source Attribution

#### Feedback

Facts about documentation, coding, and refactoring time reductions lack specific section references, page numbers, or methodology details from the McKinsey study. This makes them unverifiable.

#### Coaching

Add precise attribution: "McKinsey study (Section 3, Figure 2) shows documentation completion in 50% less time using GitHub Copilot across 100+ developers over 6 months." Include methodology details to enable verification and replication.

### Productivity Metrics

#### Issue 2: Vague Quantification

#### Feedback

Claims like "nearly half the time" and "nearly two-thirds the time" are imprecise and don't specify task types, developer experience levels, or tool configurations that affect these metrics.

#### Coaching

Specify context: "Junior developers using Copilot completed CRUD operation documentation 47% faster, while senior developers saw only 23% improvement due to existing efficiency." Include boundary conditions where these metrics apply.

## DOK2 Facts

### Missing DOK2 Summaries

#### Issue 1: No Logical Synthesis

#### Feedback

No summaries exist to connect the individual facts into coherent explanations of HOW or WHY AI tools reduce onboarding time. Facts exist in isolation without causal relationships.

#### Coaching

Create summaries connecting your facts: "AI tools accelerate onboarding because they excel at pattern recognition in structured tasks (documentation, code generation) but struggle with contextual understanding (business domain mapping, architecture decisions). This explains why routine tasks show 50% improvement while complex architecture exploration shows minimal gains."

## DOK3 Insight

### "Most grunt tasks can be reduced up to 50% completion time due to help of LLM"

#### Issue 1: Generic Marketing Statement

#### Feedback

This "insight" is marketing speak that could apply to any AI productivity use case. It lacks the specificity, counterintuitive nature, or practical application that defines genuine insights.

#### Coaching

Develop actual insights based on onboarding patterns: "AI tools create onboarding paradox - they accelerate individual task completion but slow overall architecture comprehension because engineers focus on tool interaction instead of system understanding." This provides actionable guidance for tool implementation.

## DOK4 SPOVs

### Missing DOK4 SPOVs

#### Issue 1: No Decision Framework

#### Feedback

The entire DOK4 section is empty, meaning this BrainLift provides no controversial positions or actionable decision rules for AI tool selection or onboarding methodology.

#### Coaching

Develop controversial stances like: "Prioritize AI pair programming over documentation tools - engineers learn architecture faster through assisted coding than through AI-generated documentation." Make it debatable and actionable for tool selection decisions.

---

## Overall Assessment

### Critical Recommendation: Reconsider This BrainLift's Necessity

This BrainLift appears to be a placeholder rather than a serious expertise development effort. The fundamental issues suggest either abandonment or major reconceptualization.

### Existential Questions:

1. **Why not use existing solutions?** GitHub Copilot, Cursor, and Cody already address developer onboarding. What unique value does custom expertise provide?

2. **What's Trilogy's differentiation?** Without specific organizational constraints or unique requirements, this duplicates widely available tools and documentation.

3. **Is this solvable through expertise?** Developer tool adoption is typically handled through pilot programs and training, not custom knowledge development.

### Fundamental Problems:

- **No Clear Problem Definition**: "Onboarding toil" isn't measured or specified
- **Missing Knowledge Foundation**: One source with basic facts doesn't enable expertise
- **Generic Approach**: Could apply to any company adopting AI developer tools
- **No Controversial Positions**: Nothing challenges conventional wisdom about AI tool adoption

### Alternative Recommendations:

**Option 1: Narrow the Focus**
Instead of general AI onboarding, focus on specific constraints:
- "AI-assisted legacy code exploration for COBOL systems"
- "Compliance-aware AI tools for regulated development environments"
- "AI onboarding for distributed teams across 12 time zones"

**Option 2: Pivot to Implementation Study**
Rather than building expertise, conduct practical research:
- Pilot 3 AI tools with new hires over 6 months
- Measure specific onboarding metrics (time to first PR, architecture quiz scores)
- Document what works versus what doesn't in Trilogy's specific context

**Option 3: Abandon This BrainLift**
Redirect effort toward higher-value expertise areas where Trilogy faces unique challenges not addressed by existing solutions.

### If Continuing:

**Week 1**: Define specific, measurable onboarding problems unique to Trilogy
**Week 2**: Research 10+ domain experts with conflicting viewpoints on AI tool effectiveness
**Week 3**: Develop 3-5 controversial positions about AI tool selection and implementation
**Week 4**: Create practical decision frameworks based on Trilogy's actual constraints

### Success Indicators:

**This BrainLift is working when:**
- Engineering managers cite your specific tool selection criteria
- New hire onboarding metrics improve measurably
- Your controversial positions generate debate in developer communities
- Trilogy's approach becomes a case study for AI-assisted onboarding

**Current Reality:**
This BrainLift provides no actionable value and duplicates freely available resources. The effort would be better invested in practical tool trials or more differentiated expertise areas.

Consider whether 20-40 hours of research would be better spent on implementation rather than knowledge development when the problem may not require custom expertise.
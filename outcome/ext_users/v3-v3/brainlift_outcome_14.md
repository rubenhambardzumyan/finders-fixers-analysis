# ðŸ§ BrainLift Template SV

**Owner:** Shubhashish Verma
**Assessment Date:** 2025-08-29

---

## Purpose Statement

### Issue 1: No Clear Problem Statement

#### Feedback

The purpose describes what you're capturing ("my expertise in building...") but doesn't identify the specific problem this expertise solves. Why does the world need another expert in LLM-integrated editorial workflows? What's broken that you're fixing?

#### Coaching

Reframe around a measurable problem: "Editorial teams waste 60% of time on manual content QA that LLMs could handle, but current tools hallucinate brand voice 40% of the time." This problem justifies developing expertise beyond existing resources.

### Issue 2: Missing Business Metrics

#### Feedback

The purpose mentions "ship reliable features at scale" but provides no metrics. How much faster? What reliability percentage? Without numbers, this is a resume statement, not a BrainLift purpose.

#### Coaching

Add three metrics: 1) Reduce editorial revision cycles from X to Y hours, 2) Achieve 95% brand voice consistency (current: 60%), 3) Cut content production costs by $Z per article. Track these to prove your expertise delivers value.

## Experts

### Chris Duffey

No issues found for this expert!

### Karen Hao

### Issue 1: Weak Connection to Purpose

#### Feedback

Karen Hao's ethical perspectives are listed but not connected to how they specifically help solve editorial workflow problems. Following her for general AI ethics doesn't build targeted expertise.

#### Coaching

Connect her views to your domain: "Karen's work on AI transparency directly informs how I design audit trails for editorial decisions - her WSJ piece on explainable AI shaped my DOK3 insight about auditing."

### Shubhashish Verma

### Issue 1: Self-Reference Without External Validation

#### Feedback

Listing yourself as an expert with only GitHub repos as evidence lacks external validation. This creates an echo chamber rather than expanding expertise boundaries.

#### Coaching

Replace self-reference with external validators: Who uses your tools? What metrics improved? Add testimonials or case studies showing your expertise impact, not just code repositories.

## DOK1 Facts

### Anthropic's Claude can be instructed to adopt editorial guidelines

No issues found for this fact!

### Content platforms using OpenAI's GPT-4-Turbo see reduced editing time

### Issue 1: Vague Quantification

#### Feedback

"Up to 30%" is a best-case scenario, not a reliable fact. What's the average? What conditions enable 30% vs 5%? This fact lacks precision needed for decision-making.

#### Coaching

Specify conditions: "30% reduction when using domain-specific prompt templates with 500+ training examples (OpenAI Enterprise Report, p.47)." This precision makes the fact actionable.

## DOK2 Summaries

### AI-driven editorial workflows consistently benefit from structured prompt designs

### Issue 1: Generic Summary Without Causal Explanation

#### Feedback

The summary states benefits exist but doesn't explain WHY structured prompts work better than unstructured ones. It lists observations without revealing the mechanism.

#### Coaching

Add causality: "Structured prompts reduce hallucination because they force LLMs to follow decision trees rather than probabilistic generation - the constraint paradoxically increases creativity within boundaries." This explains the HOW.

## DOK3 Insights

### Predictability Emerges from Human-First, Not Model-First Design

No issues found for this insight!

### Integration Quality Defines Editorial AI Success More Than Model Quality

### Issue 1: Missing Controversial Element

#### Feedback

This insight that "integration matters more than model quality" is widely accepted in enterprise software. It's good practice, not a surprising pattern that challenges assumptions.

#### Coaching

Make it controversial: "Teams should deliberately choose weaker models with better APIs over stronger models - GPT-3.5 with perfect CMS hooks beats GPT-4 with manual copy-paste." This challenges the "latest model" mindset.

### Off-Tone Content Is More Dangerous Than Factual Errors

No issues found for this insight!

### Auditing Beats Training for Long-Term Editorial Alignment

### Issue 1: Lacks Cross-Domain Transfer

#### Feedback

This insight stays within AI systems without connecting to patterns in other domains that could enrich understanding. Where else does auditing beat training?

#### Coaching

Add transference: "Like financial compliance where audits prevent drift better than training, or manufacturing where inspection beats worker education - continuous measurement beats initial programming." This broadens applicability.

## DOK4 SPOVs

### AI Content Systems MUST Be Designed Around Human Editorial Judgment

### Issue 1: Prediction Without Evidence

#### Feedback

"By 2026, editorial content systems... will reduce revision cycles by over 40%" is a prediction without supporting data or methodology. This weakens the SPOV's credibility.

#### Coaching

Ground in current evidence: "Systems with human approval loops already show 40% faster cycles (Contently case study) - this will become table stakes, not competitive advantage." Base future claims on present data.

### Tool Interoperability MUST Outweigh Model Sophistication

### Issue 1: Not Spiky Enough

#### Feedback

Preferring integration over raw power is standard enterprise wisdom. Any IT architect would agree. This isn't controversial enough to be a true SPOV.

#### Coaching

Sharpen the spike: "Teams should refuse to adopt ANY model - even AGI - that can't integrate with their CMS within 2 hours. Integration speed is the only metric that matters." This forces uncomfortable prioritization.

### AI Tone Alignment CAN NEVER Be Fully Trusted Without Human Auditing

### Issue 1: Absolutist Without Nuance

#### Feedback

"NEVER" and "ALWAYS" make this SPOV brittle. What about low-stakes content? Internal memos? This lacks the nuance needed for practical application.

#### Coaching

Add decision boundaries: "For revenue-generating content, accept 10% throughput reduction for 100% human tone review. For internal content, let AI run free." This makes the SPOV actionable across contexts.

---